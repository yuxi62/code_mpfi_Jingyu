# LC_HPC_manuscript analysis code

Analysis code for LC_HPC_manuscript, including dLight and drug infusion calcium two-photon imaging.

## Overview

## Code Structure

```
code_mpfi_Jingyu/
├── common/                          # Shared utility functions
│   ├── __init__.py
│   ├── plotting_functions_Jingyu.py # visualization library
│   ├── utils_basic.py               # basic utilities, trace filtering
│   ├── utils_imaging.py             # imaging data utilities
│   ├── utils_behaviour.py           # behavioral data utilities
│   ├── trial_selection.py           # trial classification functions
│   ├── robust_sd_filter.py          # robust sd based trace filtering
│   ├── event_response_quantification.py  # event-aligned response analysis
│   ├── plot_single_trial_function.py     # single trial visualization
│   ├── shuffle_funcs.py             # statistical shuffling
│   └── mask/                        # ROI mask utilities
│       ├── generate_masks.py
│       ├── neuropil_mask.py
│       └── utils_mask.py
│
├── drug_infusion/                   # GCaMP drug infusion pipeline
│   ├── __init__.py
│   ├── rec_lst_infusion.py          # recording session metadata (80+ sessions)
│   ├── session_metadata.py          # session info management
│   ├── gcamp_signal_extraction.py   # extract GCaMP signals from across ROIs across session
│   ├── process_calcium_traces.py    # dF/F calculation pipeline
│   ├── utils_infusion.py            # Drug response analysis utilities
│   ├── plot_functions.py            # plotting functions exclusive for drug infusion data
│   ├── plot_pyrUp_Down_stats.py     # pyrUp and pyrDown cell response statistics
│   ├── plot_pyrUp_Down_single_session.py  # single session plot
│   ├── plot_behaviour_trace.py      # behavioral trace visualization
│   ├── run-suite2p_GCaMP.py         # movie registration and ROI detection by suite2p
│   └── defunc/                      # will be deleted
│
├── dlight_imaging/                  
│   ├── __init__.py
│   ├── session_selection.py         # recordong seleciton for Dbh-dLight
│   ├── session_selection_geco.py    # recordong seleciton for GECO-dLight
│   │
│   ├── Dbh_dlight/                  # Dbh-axond Light imaging
│   │   ├── recording_list.py        # session metadata
│   │   ├── plot_grid_single_session_profile.py
│   │   ├── plot_grid_population_profile.py
│   │   ├── plot_dilated_grid_stat.py
│   │   ├── plot_dlightUp_grid_number_significance.py
│   │   ├── decay_time_fitting.py    
│   │   └── whole_FOV_correlation.py
│   │
│   ├── geco_dlight/                 # GECO + dLight imaging
│   │   ├── recording_list.py
│   │   └── plot_roi_population_profile_geco.py
│   │
│   ├── regression/                  
│   │   ├── __init__.py
│   │   ├── align_beh_imaging.py     # behavior-imaging alignment
│   │   ├── utils_regression.py      # regression utilities
│   │   ├── regression_axon_dlight.py
│   │   ├── regression_geco_dlight.py
│   │   ├── run_response_stats_axon_dlight.py
│   │   ├── run_response_stats_geco_dlight.py
│   │   └── utils_regression_geco/   # GECO-specific regression utilities
│   │       ├── Extract_dlight_masked_GECO_ROI_traces.py
│   │       ├── Regression_Red_From_Green_ROIs_geco.py
│   │       └── Regression_Red_From_Green_ROIs_Single_Trial_geco.py
│   │
│   └── run-suite2p-*.py             # Suite2P configuration scripts
│
└── requirements.txt
```
## Methods

### Recording Selection

Recordings with good behavioral performance and minimal movement artifacts were selected for analysis.

```python
MIN_LICK_IDX = 0.85       # Lick selectivity index
MIN_MAX_SPEED = 55        # Speed (cm/s)
MIN_PERC_VALID = 0.6      # Percentage of valid trials
MAX_TRIAL_LENGTH = 6000   # Trial length: run onset to next trial's run onset (ms)
MAX_SIG_SPEED_CORR = 0.3  # Maximum correlation with speed, to exclude sessions with clear movement artifacts
```

- `SIG_SPEED_CORR`: Correlation between the whole FOV dLight signal and running speed, calculated by [`whole_FOV_correlation.py`](dlight_imaging/Dbh_dlight/whole_FOV_correlation.py).
- Other criteria are generated by [`session_selection.py`](dlight_imaging/session_selection.py).

### Region of Interest (ROI) Detection

#### GECO ROIs

GECO ROIs were detected using the pretrained model `cyto` from the Cellpose algorithm, based on an enhanced FOV image (mean image filtered with a min-max filter).

#### GCaMP ROIs

GCaMP ROIs were detected by the Suite2P cell detection algorithm based on activity, using the following settings:

```python
ops['sparse_mode'] = True
ops['denoise'] = 0
ops['spatial_scale'] = 0
ops['connected'] = 1
ops['threshold_scaling'] = 1.0
ops['max_overlap'] = 0.75
ops['max_iterations'] = 20
ops['high_pass'] = 100.0
ops['spatial_hp_detect'] = 25
```

#### dLight and GECO Membrane Mask

The dLight and GECO membrane masks were created using the function [`generate_adaptive_membrane_mask()`](common/mask/utils_mask.py) in `utils_mask.py`.

The membrane mask was used to identify pixels with dLight or GECO expression:
1. The dLight mean image was smoothed with a Gaussian filter (sigma=1.5).
2. The coefficient of variation of 30×30 pixel image blocks was calculated across the FOV to divide the FOV into cell layer and non-cell layer. The cell layer contains somas showing dLight membrane expression; the non-cell layer has dense dLight expression on dendrites/axons.
3. The cell layer membrane mask was generated using watershed segmentation (`skimage.segmentation.watershed`).
4. The non-cell layer mask was generated using dynamic thresholding with `skimage.filters.threshold_local(block_size=5, method='gaussian')` to filter out small local dark regions.

#### dLight and GECO Neuropil Mask

The neuropil mask is defined as a square region surrounding axon or soma ROIs. The neuropil region starts 3 pixels away from the ROI mask. In each iteration, the neuropil square expands in all 4 directions by 5 pixels until the total pixel count reaches `min_neuropil_pixels`. See function [`create_neuropil_masks()`](common/mask/neuropil_mask.py) in `neuropil_mask.py`.

#### dLight Background Mask

Function: [`dlight_regressor_mask()`](common/mask/utils_mask.py) in `utils_mask.py`.

- **For dLight imaging with DBH axons**: The FOV (512×512 pixels) was divided into a 32×32 grid (16 pixels each). The dLight background mask consists of pixels with dLight expression within a 3-pixel binary dilation region (`scipy.ndimage.binary_dilation`, iterations=3) around the axon mask within each grid, excluding the original axon mask.
- **For dLight imaging with GECO**: The dLight background mask consists of pixels with dLight expression within a 3-pixel binary dilation region around each GECO ROI, excluding the original ROI mask.

#### Axon Mask

*Description to be added.*

#### Axon dLight Regression

1. The FOV (512×512 pixels) was divided into a 32×32 grid (16 pixels each). dLight signals were extracted from axon masks with dLight expression within each grid and its neuropil mask. tdTomato signals were extracted from axon masks within each grid. See function [`traces_extraction_parallel()`](dlight_imaging/regression/utils_regression.py) in `utils_regression.py`.

2. Extracted dLight signals were corrected by subtracting dLight neuropil signals multiplied by a factor of 0.2:
   ```python
   dlight_corr = original_dlight_traces - 0.2 * dlight_neu
   ```

3. A linear regression between the dLight signals and (dLight background signals + axon tdTomato signals) was used to correct for potential movement artifacts. See function [`single_trial_regression_parallel()`](dlight_imaging/regression/utils_regression.py). The fitting was performed for each trial segment (90 frames pre- and 120 frames post-run-onset). After obtaining the coefficients for the dLight background signals (green_regressor) β₁ and tdTomato signals (red_regressor) β₂, the artifact was estimated from the time-varying components of the two regressors and subtracted from the neuropil-corrected dLight signals:
   ```python
   artifact_est = β1 * (green_regressor - np.nanmean(green_regressor)) + β2 * (red_regressor - np.nanmean(red_regressor))

   dlight_clean = dlight_corr - artifact_est
   ```

#### GECO dLight Regression

1. GECO ROI and neuropil signals were extracted by Suite2P from masks described in the previous section ([GECO ROIs](#geco-rois)).

2. For each GECO ROI, the dLight signal was extracted from the ROI mask with dLight expression.

3. Extracted GECO signals were corrected by subtracting GECO neuropil signals multiplied by a factor of 0.7. Extracted dLight signals were corrected by subtracting dLight neuropil signals multiplied by a factor of 0.2:
   ```python
   geco_corr = original_geco_traces - 0.7 * geco_neu

   dlight_corr = original_dlight_traces - 0.2 * dlight_neu
   ```

4. GECO signals below (trace median + 2 × trace robust SD) were considered GECO baseline signals, which contain potential movement artifacts.

5. A linear regression between the dLight signals and (dLight background signals + GECO baseline signals) was used to correct for potential movement artifacts. See function [`run_and_save_motion_correction_results()`](dlight_imaging/regression/utils_regression_geco/Regression_Red_From_Green_ROIs_Single_Trial_geco.py) in `Regression_Red_From_Green_ROIs_Single_Trial_geco.py`. The fitting was performed for each trial segment (90 frames pre- and 120 frames post-run-onset; segments with fewer than 100 GECO baseline datapoints were extended until 100 datapoints were included). After obtaining the coefficients for the dLight background signal (green_regressor) β₁ and GECO baseline signals (red_regressor) β₂, the artifact was estimated from the time-varying components of the two regressors and subtracted from the neuropil-corrected dLight signal:
   ```python
   artifact_est = β1 * (green_regressor - np.nanmean(green_regressor)) + β2 * (red_regressor - np.nanmean(red_regressor))

   dlight_clean = dlight_corr - artifact_est
   ```

### Drug Infusion Analysis

*Description to be added.*


## Installation

### Requirements
- Python 3.8+
- NVIDIA GPU with CUDA support (recommended for large datasets)

### Setup

```bash
# Clone the repository
git clone https://github.com/yuxi62/code_mpfi_Jingyu.git
cd code_mpfi_Jingyu

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# For GPU acceleration (adjust CUDA version as needed)
pip install cupy-cuda11x
```

## Dependencies

| Package | Purpose |
|---------|---------|
| NumPy | Numerical computing |
| Pandas | Data manipulation & parquet I/O |
| SciPy | Scientific computing |
| CuPy | GPU-accelerated arrays |
| OpenCV | Image processing |
| Matplotlib | Visualization |
| scikit-image | Image analysis |
| xarray | NetCDF data loading |
| tqdm | Progress bars |

## Author

Jingyu Cao
Max Planck Florida Institute for Neuroscience

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Citation

If you use this code in your research, please cite:
```
[Citation information to be added upon publication]
```

## Acknowledgments

- Max Planck Florida Institute for Neuroscience
- Wang Lab
